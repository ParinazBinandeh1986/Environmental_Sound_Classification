{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYqPiPg8c50Ud/GBpqBDSy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParinazBinandeh1986/Environmental_Sound_Classification/blob/main/MAFnet_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ§  MAFnet Full Implementation Pipeline for AVE Dataset\n",
        "# Based on the paper:\n",
        "# \"Multi-Level Attention Fusion Network for Audio-Visual Event Recognition\"\n",
        "# --------------------------------------------------------------\n",
        "# This Colab notebook guides you through the complete process of reproducing\n",
        "# the MAFnet model for audio-visual event recognition, step by step.\n",
        "# It includes dataset setup, feature extraction, model implementation,\n",
        "# training, and final visualization.\n",
        "# --------------------------------------------------------------"
      ],
      "metadata": {
        "id": "U27g3_K6dB8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 1ï¸âƒ£ Mount Google Drive and Define Paths\n",
        "# ============================================================\n",
        "# Mount your Google Drive to access the AVE dataset and save model outputs.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "PROJECT_PATH = \"/content/drive/MyDrive/MAFnet\"\n",
        "DATASET_PATH = \"/content/drive/MyDrive/AVE\"\n",
        "FEATURE_PATH = f\"{DATASET_PATH}/features\"\n",
        "LABELS_CSV   = f\"{DATASET_PATH}/labels.csv\"\n",
        "\n",
        "os.makedirs(PROJECT_PATH, exist_ok=True)\n",
        "os.makedirs(FEATURE_PATH, exist_ok=True)\n",
        "print(\"âœ… Project folders ready. You can now proceed with feature extraction and model training.\")"
      ],
      "metadata": {
        "id": "1Czvw6fXdesl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 2ï¸âƒ£ Install Dependencies\n",
        "# ============================================================\n",
        "# Install all required Python libraries for the MAFnet pipeline.\n",
        "!pip install tensorflow==2.17.0 librosa vggish opencv-python tqdm matplotlib numpy pandas scikit-learn"
      ],
      "metadata": {
        "id": "mZgif6vadq7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 3ï¸âƒ£ Import Libraries\n",
        "# ============================================================\n",
        "# Load all necessary libraries for data processing, modeling, and visualization.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa, cv2, os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.applications.densenet import preprocess_input\n",
        "from tqdm import tqdm\n",
        "from vggish import VGGish\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "WiDFc051eB7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 4ï¸âƒ£ Feature Extraction (Visual + Audio)\n",
        "# ============================================================\n",
        "# Extract features using DenseNet (for visual) and VGGish (for audio) as described in Section 4.2 of the paper.\n",
        "\n",
        "# ---- Visual extractor ----\n",
        "vis_model = DenseNet121(weights='imagenet', include_top=False, pooling='avg')\n",
        "vis_model.trainable = False\n",
        "\n",
        "def extract_visual_features(video_path, T=10, frames_per_clip=16):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS)) or 25\n",
        "    clip_len = total_frames // T\n",
        "    features = []\n",
        "    for t in range(T):\n",
        "        start = t * clip_len\n",
        "        frames = []\n",
        "        for i in range(frames_per_clip):\n",
        "            frame_id = int(start + i * clip_len / frames_per_clip)\n",
        "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
        "            ret, frame = cap.read()\n",
        "            if not ret: continue\n",
        "            frame = cv2.resize(frame, (224, 224))\n",
        "            frame = preprocess_input(frame.astype(np.float32))\n",
        "            frames.append(frame)\n",
        "        if len(frames) > 0:\n",
        "            frames = np.array(frames)\n",
        "            clip_feat = vis_model.predict(frames, verbose=0)\n",
        "            clip_feat = np.mean(clip_feat, axis=0)\n",
        "            features.append(clip_feat)\n",
        "    cap.release()\n",
        "    return np.array(features)\n",
        "\n",
        "# ---- Audio extractor ----\n",
        "vggish_model = VGGish()\n",
        "vggish_model.trainable = False\n",
        "\n",
        "def extract_audio_features(video_path, T=10):\n",
        "    tmp_audio = \"/tmp/temp_audio.wav\"\n",
        "    os.system(f\"ffmpeg -loglevel quiet -y -i '{video_path}' -vn -acodec pcm_s16le -ar 16000 -ac 1 {tmp_audio}\")\n",
        "    y, sr = librosa.load(tmp_audio, sr=16000)\n",
        "    total_len = len(y)\n",
        "    clip_len = total_len // T\n",
        "    feats = []\n",
        "    for t in range(T):\n",
        "        seg = y[t*clip_len:(t+1)*clip_len]\n",
        "        if len(seg) < 1000: continue\n",
        "        mel = librosa.feature.melspectrogram(y=seg, sr=sr, n_mels=96)\n",
        "        mel = librosa.power_to_db(mel)\n",
        "        mel = np.expand_dims(mel, axis=(0, -1))\n",
        "        audio_feat = vggish_model.predict(mel, verbose=0)\n",
        "        feats.append(audio_feat[0])\n",
        "    return np.array(feats)\n",
        "\n",
        "# ---- Process dataset ----\n",
        "def process_AVE_dataset(video_dir, save_dir):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    for video_name in tqdm(os.listdir(video_dir)):\n",
        "        if not video_name.endswith('.mp4'):\n",
        "            continue\n",
        "        base = os.path.splitext(video_name)[0]\n",
        "        vpath = os.path.join(video_dir, video_name)\n",
        "        try:\n",
        "            vfeat = extract_visual_features(vpath)\n",
        "            afeat = extract_audio_features(vpath)\n",
        "            np.save(os.path.join(save_dir, f\"{base}_visual.npy\"), vfeat)\n",
        "            np.save(os.path.join(save_dir, f\"{base}_audio.npy\"),  afeat)\n",
        "        except Exception as e:\n",
        "            print(\"Error processing\", video_name, \":\", e)"
      ],
      "metadata": {
        "id": "HJG9pVM7eHys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 5ï¸âƒ£ Build the MAFnet Architecture\n",
        "# ============================================================\n",
        "# Build the core MAFnet structure including FiLM, Temporal Attention, and Modality Attention.\n",
        "\n",
        "class FiLM(layers.Layer):\n",
        "    def __init__(self, feature_dim):\n",
        "        super().__init__()\n",
        "        self.gamma_dense = layers.Dense(feature_dim)\n",
        "        self.beta_dense  = layers.Dense(feature_dim)\n",
        "    def call(self, inputs):\n",
        "        audio_feat, visual_feat = inputs\n",
        "        gamma = self.gamma_dense(visual_feat)\n",
        "        beta  = self.beta_dense(visual_feat)\n",
        "        return gamma * audio_feat + beta\n",
        "\n",
        "class TemporalAttention(layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.fc = layers.Dense(units, activation='relu')\n",
        "        self.score = layers.Dense(1)\n",
        "    def call(self, features):\n",
        "        x = self.fc(features)\n",
        "        s = tf.nn.softmax(self.score(x), axis=1)\n",
        "        out = tf.reduce_sum(s * features, axis=1)\n",
        "        return out, s\n",
        "\n",
        "class ModalityAttention(layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.fc = layers.Dense(units, activation='relu')\n",
        "        self.score = layers.Dense(1)\n",
        "    def call(self, modality_feats):\n",
        "        stacked = tf.stack(modality_feats, axis=1)\n",
        "        x = self.fc(stacked)\n",
        "        s = tf.nn.softmax(self.score(x), axis=1)\n",
        "        out = tf.reduce_sum(s * stacked, axis=1)\n",
        "        return out, s\n",
        "\n",
        "def build_mafnet(T=10, feature_dim=512, num_classes=28):\n",
        "    visual_inp = layers.Input(shape=(T, 1920))\n",
        "    audio_inp  = layers.Input(shape=(T, 128))\n",
        "    v_proj = layers.TimeDistributed(layers.Dense(feature_dim, activation='relu'))(visual_inp)\n",
        "    a_proj = layers.TimeDistributed(layers.Dense(feature_dim, activation='relu'))(audio_inp)\n",
        "    film = FiLM(feature_dim)\n",
        "    a_film = layers.TimeDistributed(film)([a_proj, v_proj])\n",
        "    t_v = TemporalAttention(feature_dim)\n",
        "    t_a = TemporalAttention(feature_dim)\n",
        "    v_sum, _ = t_v(v_proj)\n",
        "    a_sum, _ = t_a(a_film)\n",
        "    m_att = ModalityAttention(feature_dim)\n",
        "    fused, _ = m_att([v_sum, a_sum])\n",
        "    x = layers.Dense(512, activation='relu')(fused)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    out = layers.Dense(num_classes, activation='softmax')(x)\n",
        "    return models.Model([visual_inp, audio_inp], out)"
      ],
      "metadata": {
        "id": "zi40VC54eRTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# STEP 6ï¸âƒ£ Load Features and Train Model\n",
        "# ============================================================\n",
        "def load_features_and_labels(feature_dir, label_csv, num_classes=28):\n",
        "    df = pd.read_csv(label_csv)\n",
        "    Xv, Xa, y = [], [], []\n",
        "    for _, row in df.iterrows():\n",
        "        name, label = row['video'], row['label']\n",
        "        try:\n",
        "            v = np.load(os.path.join(feature_dir, f\"{name}_visual.npy\"))\n",
        "            a = np.load(os.path.join(feature_dir, f\"{name}_audio.npy\"))\n",
        "            Xv.append(v)\n",
        "            Xa.append(a)\n",
        "            y.append(label)\n",
        "        except: continue\n",
        "    Xv, Xa = np.array(Xv), np.array(Xa)\n",
        "    y = tf.keras.utils.to_categorical(y, num_classes)\n",
        "    return Xv, Xa, y\n",
        "\n",
        "# Example training (uncomment below to run):\n",
        "# Xv, Xa, y = load_features_and_labels(FEATURE_PATH, LABELS_CSV, num_classes=28)\n",
        "# model = build_mafnet()\n",
        "# model.compile(optimizer=optimizers.Adam(1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# history = model.fit([Xv, Xa], y, batch_size=8, epochs=50, validation_split=0.1)\n",
        "# model.save(f\"{PROJECT_PATH}/MAFnet_trained_AVE.h5\")\n",
        "# print(\"âœ… Model trained and saved!\")"
      ],
      "metadata": {
        "id": "S-NJ73HLefOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3gKJxSLbcEd"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 7ï¸âƒ£ Visualize Results using t-SNE\n",
        "# ============================================================\n",
        "# Visualize the learned multimodal embeddings before and after the FiLM layer to verify cluster separation.\n",
        "def tsne_visualization(model, Xv, Xa, y_labels):\n",
        "    extractor = models.Model(inputs=model.inputs, outputs=model.get_layer(index=-3).output)\n",
        "    features = extractor.predict([Xv, Xa], verbose=0)\n",
        "    tsne = TSNE(n_components=2).fit_transform(features)\n",
        "    plt.figure(figsize=(8,6))\n",
        "    scatter = plt.scatter(tsne[:,0], tsne[:,1], c=np.argmax(y_labels, axis=1), cmap='tab20', s=10)\n",
        "    plt.title(\"t-SNE Visualization of Fused Audio-Visual Features\")\n",
        "    plt.colorbar(scatter)\n",
        "    plt.show()\n",
        "\n",
        "print(\"âœ… MAFnet full Colab pipeline ready to run! Follow the steps above to train and visualize results.\")\n"
      ]
    }
  ]
}